---
title: "Creating A Content Database With Claude â€“ Overcoming Upgraded Skill-Level Problems And Limitations (Cleaned Transcript)"
---

## Intro

[00:00:03]
**Bradley:**
Alright. Frankly, okay. Frankly, don't know why I'm necessarily making this voice note right now. I just, honestly, I just needed to get out on a walk, clear my mind. Maybe I'll conversationally figure out some of the like, problems or limitations and obstacles I'm currently facing right now, which let's be real, they're not, they're not crazy problems or like obstacles.

There's definitely limitations, I think. I don't know, the thing that I'm struggling with, and this is kind of a meta thing too, is just like, but I would say. So how do I say this? How do I get this voice note to be clearer too? Like, that's one thing I'd like to do.

[00:01:18]
**Bradley:**
Alright, this is test angle 1. This is test angle 2. This is test angle 3.

[00:01:28]
**Bradley:**
This is test angle 4.

[00:01:34]
**Bradley:**
Sorry, let me try that real quick. So this is test angle 1. This is holding the speaker on my iPhone like above my mouth, below my nose, okay, test angle 2. This is below my mouth and my nose, okay, test angle 3 is holding it just out like 6 inches away from my nose and my mouth. And then test angle 4, this is just holding it at my chest.

[00:02:06]
**Bradley:**
Chest level.

[00:02:09]
**Bradley:**
Okay, I think we're gonna go with test angle 2, which is totally below my mouth and my nose, basically at my chin level. Hopefully that doesn't get all the loud breathy noises, but also hopefully avoid some of like the walking wind noises as well. We'll see.

[00:02:31]
**Bradley:**
I think ultimately I just got to get one of those DJI like, speaker, what are those things called, man?

[00:02:39]
**Bradley:**
Whatever. Those things are like the speaker kind of windbreaker things that you can plug into the, like the USB C port of your phone and just canceled all wind noise, like those little fluffy things. Anyway, I'll get one of those one day. But okay, so hopefully I can conversationally solve a lot of these things, I think. So here's the deal.

## Exploring Skill Upgrades and Problem-Solving Approaches

[00:03:05]
**Bradley:**
This, this is sort of the meta problem that I'm facing, right, is whenever, like whenever I'm approaching a new skill, right? That often, oftentimes entails like a new sort of, I guess there's like a new goal or whatever, but there's always a new method or approach or process and I guess you could say a different unique mechanism or a unique system that you could use. I'm just going to call this a unique mechanism. You know what it is, right? Like oftentimes this is relevant to sales pages too.

When you're selling someone like a product or system, product or service, whatever. But I'm going to talk about in my context in terms of approaching a new skill or whatever I'm doing. Oftentimes a goal might remain the same, but the skill I'm upgrading. And so I'm presented with a multitude of multiple different paths that I could take because there's never just one method, there's never just one path or process that you could use to ultimately get to the solution, which the solution is almost the same every time. But I guess you could even have varying degrees of solution by like the quality of solution, the format of the solution, you know, what kind of asset you end up with, how easy and user friendly that solution is to use.

So anyway, but the path or the unique mechanism is always where there's the most variance.

[00:04:48]
**Bradley:**
And the problem I face, like a lot of times when I'm consulting Arno, either via Claude by Anthropic, again, Claude / Claude by Anthropic or ChatGPT, either one is going to be Arno. But I, a lot of times I'll ask, "Hey, here's basically my goal and my background context. Here's the purpose, what I'm trying to achieve ultimately, you know, this is sort of the output or like the final, yeah, like the final form output that I'm trying to walk away with. But I don't know what would be the most like practical, effective and user friendly method or process to do that. I don't know what would be the best bridge, right?"

What would be the best path or bridge to go from point A to point B. And oftentimes I'll have Arno say, "Okay, cool, like give me, you know, 3 or more different potential approaches that we could use and then kind of give me some of the pros and cons of each, keeping in mind my context, like my skill level, sort of the background skills that I have, what level I'm at. So if I'm going to involve programming, for example, I obviously want Arno to know first off what level I'm at with SQL. Like I'm a beginner programmer, right? What languages I'm familiar with Python and just a tiny bit of JavaScript, what tools I'm going to be using, right?"

And like environment, that kind of stuff. So I'm going to be using either VS Code or Cursor on my MacBook Pro and I'm going to be using local files, for example, right? So all of that is back usual background context and sort of criteria that will guide the method or approach that makes most sense. But I think the next sort of criteria is. Okay, how, like how hands on do you want this to be?

Do you, are you familiar with the way that this type of programming script will work and the libraries you need to import for it? How programming heavy is this? Or are you aiming for more of a user friendly hands off approach?

Maybe the con, or like the trade off of that is you might have to do more manual copying and pasting. But the pro, like the upside is you won't be getting lost, like neck deep in Python code that you don't understand. Right. So yeah, I think it's important to, like, you're sort of weighing the pros and the cons based on your current context, like your current state or your current situation, and then your like target criteria. For what type of method am I looking for?

You know, I mean, this kind of goes to when you're selecting, when you're choosing a car, for example, like if I'm going to go choose a car to buy, do I just need something that's reliable? It's not going to break down. It's going to get me from point A to point B. I can run up a lot of miles on it and I'm not going to have to take it into the shop very often. And it doesn't need to be anything fancy or sporty.

It just needs to look okay, you know, like look reasonable and ultimately get me to point A, give me from point A to point B reliably and effectively and efficiently, like gas mileage wise. Right? So that's where my criteria is. You'd have a, you could have a, like anti criteria and then a. I don't know what you want to call this, a pro criteria, but it's almost like the anti vision vs vision thing, right?

So for the car selection thing, like the anti criteria would be, you know, it doesn't have to be fancy, it doesn't have to be sporty, it doesn't necessarily have to be new, right? Like you might give a year range of a year range of how new or how old you're looking for, right? Like make sure to exclude results from this old year range. And also like, you can include results that are up to the current year, but you, you don't have to, like here's sort of a better range to look for. That's more middle of the road, right?

And then your pro criteria would just be like, okay, I'm looking for something that gets this range of miles per gallon. It's efficient. Also this type of reliability, you can give examples of car manufacturers like Honda, Acura, Toyota, whatever. You don't necessarily need to give a certain score. You just need to give like a comps, like a comparable model, I guess, comparable make or model.

So, yeah, man, that's just basically what I'm trying to do is do this because I think here's the thing. And then, like, that could be your criteria for searching for a car that you want to buy. But then your methods could be totally different too, like depending on which type of dealership you want to go through, how many different cars you want to test drive.

What's your search range? Right. Like, are you willing to go to another state or do you want to look for cars that are within a 25 or 50 miles radius from where you're currently located? Right. Because that's all going to determine, like, those type of criteria are going constrain your method, your methods that are available to you in terms of how you're going to track down a car that meets the rest of your criteria and then ultimately choose one to purchase and buy.

Right.

[00:10:10]
**Bradley:**
So anyway, my thing is, so let me put this into my current context. I have just recently discovered a couple new tools or a couple new methods, I guess you could say a couple new tools that open up new methods, new, unique mechanisms to me.

And at the same time, I am coming up against new problems or obstacles with, like, where like, certain limitations are more relevant. And so, yeah, man, I'm basically, I'm upgrading my skills. I'm acquiring new skills which obviously in the pursuit of, even if you're pursuing the same goal, if you're upgrading your skills, you are probably going to be like, coming up against new problems in pursuit of that goal because, yeah, man, it's just natural. Like, as you upgrade your skills, you come up against new problems or obstacles that are unique to that level of the skill, regardless of if the North Star goal is the same. Right?

Yeah. And then obviously solutions, there's a little bit more variability in terms of what type of solution that you looking to get. Right.

[00:11:47]
**Bradley:**
But yeah, anyway, ideally the goal and the solution would be the exact same, like they've never changed, regardless of, you know, the level of skill that you're on and the types of, like, methods or unique, unique paths, unique mechanisms that you have available to you. But I do feel like the solution at least, like the form, the form of solution does sort of get impacted a little bit. But again, let's just focus on the methods, like the unique mechanism. So, so let me, let me just lay this out for you. So right now, I have.

I think I. The majority of these things come back to prompt engineering, right? The majority of. I mean, in terms of my goals and then the solutions that I'm seeking. The majority of the unique mechanism, which is like the process or the system I use to bridge that gap to overcome the current problems I'm facing.

Obstacles, limitations, whatever. I think the majority of the unique mechanisms available to me all sort of collapse and consolidate around prompt engineering, right? But given that, there are a couple other considerations that need to be addressed. So let me give you an example.

But I just want to put that out there as a note that, like, no matter what skill level I get to, no matter what new tools I have available to my. To me, even a lot of times in light of new limitations or, like, I'd say expanded limitations, where, like, the previous limitation was tighter and then now, like, the limitation gets upgraded, so you have more flexibility or more breathing room. Regardless of those. Like, the majority of problems can be solved with the unique path or unique mechanism of prompt engineering, even though that specific implementation of prompt engineering might look a little bit different depending on the exact context of your problem or the current limitations, current obstacles, that kind of stuff. But I think the reason that unique.

That new unique mechanisms or paths become available to you in the first place is because you are upgrading your awareness around the number of ways that this problem could be solved. And I'd say just naturally, like, I'd say just naturally, like, your new tools come out, right? Like people are iterating, inventing new tools, new GitHub, repos, new methods of solving old problems. And then given the fact that, like, limitations are always evolving, right. I think that's huge, too.

Like, that again, opens up. It just all compounds, right? All these factors basically multiply, multiply each other. New tools, upgraded limitations, I would say even upgraded, like, methods or infrastructure, things like prompt caching, right? Which is something that you can use via Anthropic's API that was never, I mean, it's kind of like a new tool, but it's almost like a new native tool or new feature of an existence.

So let me just share. Anyway, that's. That's the guess. How would you say that's like the meta.

[00:15:50]
**Bradley:**
That's like the meta, I guess I don't know what you want to call it, the meta, big picture sort of thing that I'm. That I'm becoming aware of and addressing for my own goals, my own unique mechanisms, processes, solutions, whatever you want to call it, but, yeah, man. Anyway, let's hopefully I don't get eaten by coyotes or something down here. I always get a little bit nervous when I'm going on the powerline road.

Cross over to the side of Powerline Road that goes up to Madera Canyon club, Madeira Club. It's all curvy, but it's also pitch black. Got about half a moon right now, and there's been California wildfire. So it's a little bit, a little bit filtered through our best. Let's go those coyotes away, man.

## Diving into Specific Challenges and Solutions

[00:17:11]
**Bradley:**
So, so let me, let me just conversationally. Brain dump, minute 18. Okay, so minute 18. Good stuff. That's where the meta stuff stops.

That's where the more detailed granular focus picture.

[00:18:38]
**Bradley:**
Okay, so basically, how do I say this?

[00:18:55]
**Bradley:**
So basically right now I'm doing something.

[00:19:00]
**Bradley:**
Let me give you, let me give you a small sample sort of like solution.

[00:19:07]
**Bradley:**
You don't always need the fanciest, most like, efficient or unique mechanism.

Ultimately, in the long run, if you are performing, you know, 5 or more like 5 to 10+ repetitions of, of this particular process, you probably want to look into more automated solution, especially if each repetition is a very long process, like a prompt change, for example.

But anyway, what I do is, I guess maybe there should be some sort of like qualification criteria that allows you to basically select the, like, the level and type of unique mechanism that you should opt for at this given stage. Again, you're given context, goals, that kind of stuff. Because, for example, yeah, anyway, I think that makes a lot of sense because, for example, I asked Arno, I was like, "Hey, man, I have, I'm trying to produce for my client's content database. This is Laney's content database. I need to basically download every single blog post from her website as a markdown file."

And I need to like download them to one specific folder and find her on my laptop. And I asked him to suggest, you know, at least 3 different methods or approaches we could take to doing this. And he, I expected him to like to recommend web scraping as more of like, as a higher priority option. But he said like, let's save this for last. This is definitely number 3 out of 3 recommendations.

We don't want to go overkill mode if we don't have to. And I asked him, I said, "Okay, like, I just used a Google Chrome extension, mark download to download 4 blog posts. Ended up, I thought that it was going to be 50 blog posts or so. It ended up being about 82 and so."

And so I basically asked him, I was like, "Okay, at what point would you like consider using web scraping as like a priority number 1 option? Like what? At what point would that make more sense?" And he said, honestly, and I don't know if this was just based on my ranges, I don't know if I would. If I were to just ask him that as a first message in a chat thread before revealing how many posts were, he might respond different.

But he basically said, "Well, anything from like 0 to 99 posts, I'd use a Google Chrome extension to just like download each individual blog post to markdown file and then anything from like 100 to 500 blog posts. That's more web scraping territory. And then 500+ items is definitely web scraping territory, especially if you have these other factors, such as you need to fetch new data from the website because it's being updated every day, for example, right, as opposed to blog post content, which is more static once it's posted." So that was useful to just kind of know like which methods are more, are most appropriate for which use cases and which sort of like factors or situations, right? So if there is any sort of way to like quantify, quantify certain criteria and also provide mental heuristics for each recommended approach or unique mechanism.

So those 2 things quantify some sort of like factors or criteria and then also add mental heuristics that allow you to easily choose which one makes most sense for your particular situation or use case.

So anyway, that's what I would say that'll help Arno make better recommendations for you in the future.

But, yeah, man.

[00:23:51]
**Bradley:**
But yeah, sorry, that's more meta stuff. Still kind of granular. Like a granular example. Still meta stuff. Okay, starting minute, 24 minutes.

[00:24:05]
**Bradley:**
24 is where we're getting more my personal situation right now. So currently, like the new, how do you say, the new skill or the new thing that I'm trying to, I guess the new goal I'm trying to achieve, the new output or solution I'm trying to create.

And again, this new goal introduces new problems. Point is, what I'm trying to create is I'm trying to take all of Laney's 82 existing blog posts, which again, I've already downloaded. I have like appended a little like custom acronym and also index number to each file name. So it's like for content database. So it's FCD underscore and then the number.

So like 1, 2, 3, whatever, all the way up to 82. So I can identify each unique file name and like have the AI iterate through numbers 1 through 10, 11 through 20 and so on, right?

Which I believe is smart, but I think the problem I'm coming up against is. So here, here are the methods. Right? Point blank, if I had no awareness or experience and like basic familiar familiarity and practice with using, number 1, Python. Like writing Python scripts in general, like in VS Code.

Or number 2, using Cursor. Right? Like Cursor AI. It's an app that basically operates just like VS Code, except that you have Claude 3.5 Sonnet built in both as a like composer and as like an AI chat sidebar kind of thing where I can directly edit your code. You can use a diff checker to proved individual changes or accept all changes at once.

It can beat your entire code base. It's just a much more practical solution to situations where you have multiple different files and you almost have like a different, how to call this? You have multiple different directories that all relate to each other and like integrate and connect to each other and like influence each other in certain ways. And you need the AI chat, like the AI model to understand that, to make proper recommendations for how you should approach solving your problem, right? Because it's not just one file you're dealing with.

It's not just one markdown formatted artifact file they're trying to produce or update. It is like 17 different files. Some of them are nested in different folders. Some of Python scripts affect other Python scripts, different modules affect different modules, all this kind of stuff. So anyway, point is, what am I trying to get at?

My question is, so for Laney's content database, one thing is, I am, one thing I need to figure out first off is whether I should structure this her like Notion content database as just a table where everything. Where each item exists on the main level. And what I mean by that is in a database. And Notion, you can enable the option for like parent items to have sub items, right? Otherwise, everything is just like main.

Everything is just a parent item. It's just the main level item. There's no such thing as parent or child or parent or sub item. Like just is. But the question comes into play is like, okay, what, what if for like a parent parent level item, I'm going to add the main blog post as like the Hub content, right?

Hub and spoke. So that's the main blog post / newsletter. That's the hub content. That is the parent item. And then if you want to view the spokes, right, which are the social media posts, like the social posts, you can then toggle that drop down.

Like, you can toggle that menu down and then it will have sub items or child items, which again, are the individual spokes or the individual social posts that are all based on the hub content, which is the original blog post. Newsletter. Right. But the problem is I need, and that, like, that makes most intuitive sense to me because that way you can just look at, like, your main, your main blog posts as like the, basically, like the foundation or the hub for everything else. And you don't need a, to have a bunch of unnecessary social posts cluttering up your view if you don't want to look at those.

But an alternative option, you could do a view where, like, you, or you have a property that basically specifies, okay, is this item a long form post or is it a social post? Right? Or you can even, instead of making that property long form vs short form, you could even make like, make it a content type. And then you sort of have, like, in your mind, you're grouping these long form items together where it could be newsletter, blog post, video script, podcast script, or trying to think. I think those are the main 4 long form versions, right?

Yeah. And then, so you can just select the content type. That way it sort of abstracts away long form or short forming because you're sort of delete that property because you're opting for the more specific property, which is the content type. And you inherently know whether that content type is long form or short form. And then your short form options would obviously just be like Instagram post, LinkedIn post, tweet, Twitter thread.

Yeah. I don't know. I don't know if you want to get any more specific. But anyway, you can add more short form post options that are basically the content type type. But you just specify the platform and you specify the platform and then potentially the sub format.

So kind of like, you can't just select Twitter, you'd have to select either tweet, like an individual tweet or a Twitter thread, right. Both of those options belong to the platform Twitter. But both those options are a different sub format, like an individual tweet vs full on Twitter thread. Right. And then the long form posts.

Right? So you go to that view and just long form. That view is called long form post.

And then you have another view that's just short form post, right, which shows all the spokes content. And then the long form shows the hub content. Right, which is the original newsletter or blog post, whatever you're basing this on. So that's an alternative strategy for if, like, the sub items strategy doesn't work out because here's the thing I need to figure out if I can import CSV file into Notion that has sub items, right? I think.

I think that sub items are really just like a relation property that's connected to the parent item. I'm not sure exactly how that works. I think.

I think parent item is a relation property that relates to sub item within the same database sub item and you can only select certain, and maybe only certain like items show up as being available. You wouldn't want to click to fill in parent items, I don't know.

So those are the 2 solutions if I can create a CSV file. So here's what to do. Test experiment with just create like a brand new like just fresh completely blank database and fill in a couple placeholder items and then and just make sure you have like sub items enabled. So add a few sub items and add just like 1 or 2. Just add like 2 different parent items and then parent item number 1 will have a couple sub items and parent item number 2 will not have any sub items.

And then see if you can export that because there won't be any related pages or anything. It's just a step database. Export that from Notion as a CSV file and then see if you can modify Google sheets and see if you can modify those properties and then even add more. Pair items with additional sub items and see if you can re import that back into Notion and see if that database structure the same or fit. See if it doesn't recognize sub items from the CSV file trying to import.

Then we'll just go view and also a short form view with as a rule.

Does that make sense? That makes sense and I'll easily be able to do this. The key thing here is I have to do that initial test experiment where I create a brand new like brand spanking new blank Notion database and fill a few different pages. Again, parent post number 1, post like 1.1 sub post 1.2 and then parent post number 2 with no sub posts, export that from Notion, open it in like Google sheets and then modify it by adding additional parent items and sub items. And see if I can reimport that modified version of CSV file back into Notion and see if it maintains my intended like structure and formatting and layout with all its relations between parent items and sub items.

If that does work great, I'll go ahead and export my like Laney's current content database which has this these parent items and sub items. I'll export that to a CSV file. I'll modify it in Google sheets or I'll have like a Python script or Claude, you know, write the content I need to it, modify it in Google sheets, then re import it as, yeah, just as it's supposed to be re imported as a brand new database. And then just fill in, you know, modify any additional properties, any additional connections I need to, to make it all clean.

That makes sense, man. That makes sense. So the only other main thing that I'm struggling with right now with this whole content database thing is, so that's the number 1 question is the Notion like import / export ability with CSV files and sort of that like parent item connected to sub item dependencies or like that nesting ability as it relates to importing CSV files that'll maintain that sort of structure? My second main question, / problem / obstacle. What I'm trying to face right now is, how do I say this?

Okay, here it is. So I'm trying to figure out, right, like, so I uploaded all 82 of Laney's blog posts as markdown files to Claude's like web AI chat user interface, right? And those 82 blog posts, when they're project knowledge based files, they use up like 97% of the context window for all like new chats. So basically you want to start a new chat. Great.

97% of your context window is gone because each new chat thread within that project, what Claude does is it literally reads, it literally reads through every single freaking file in your project knowledge base before it, like, answers your first message. So instead of selectively just telling which files you want it to focus on, it literally treats like all these project knowledge base files, almost like a system prompt where it instinctively like, automatically reads the entirety of all these files before you, even before it even reads your first user prompt message. So that's a problem, right? So a couple workarounds for this. I could either use prompt caching, which might make more sense in the long run, but it seems like it's just going to take too much mental energy and trial and error experiments.

Figure that out, because I'm already like having a hard time figuring out how to, how to create a freaking like, chat thread. Like basically message messages list that stores the entirety of back and forth messages between the AI assistant and the user within some sort of Python formatted list as like the messages history or conversation history. Like, I'm still trying to figure out how to do that. So this just won't work because you have to use, to use prompt caching, you have to use the Anthropic API. And for this method to even make sense, I first need to already understand how to use a conversation history list of variable and I just don't know that yet.

So I think that kind of x's out or crosses out that method as in terms of viability for what I'm going to go for right now. Plus this whole content database is due tomorrow, so that's more like an option for future, future editions, future versions. If I want to do this again in a more efficient way. The second way is let's see if I do this second way would be, let's see what I probably do. Actually, this makes more sense.

So because there's so many files, because those 82 individual blog post files take up 97% of the context window, it wouldn't make sense for me to like add 5 or 10 of them at a time to the AI chat. Because eventually by the end of the chat, like still going to burn that entire context window by the time that all 82 are added, even if I add them in individual messages. So that doesn't make sense. What I do is this from my current, here's an option, option A, okay, from my current chat thread where I've been like discussing the strategy and the approach, just basically step by step instructions for how Arno and I are going to tackle this content database. Basically, I would want him to like, let's say, "Hey man, if I'm, because all 82 of these blog posts are so long, they're gonna burn up our entire context window if we were to try to process them all in this individual chat thread."

"So what I want you to do is basically like go through all of the messages, like in this current chat history, in this current chat thread, go through all of your messages, my messages, every single message. And I want you to basically create a, like a first message or a context, like a project context message that I can copy and paste as my first user message prompt into a brand new chat thread. So that a new chat thread with you in it, with no prior context of what we talked about here, will be able to be off to the races and just hit the ground running, just given that one initial user message as context for this project, including, you know, the goal and purpose. And here's what I said. Here's the, basically the template I want you to fill out with the major sections."

I'll just boom from Snippety, like import my prompt outline that has like section 1 goal, which is like task and purpose, right? Section 2, context, background and situation, and all those major subhead sections. And I'd say, "Use this template, and I want you to, again, go through our entire current chat history, all the messages, and basically prep a."

A brief or whatever you want to call this. Tell them the situation. This is going to go into a brand new, fresh AI chat thread, where you will have no previous context of any of the messages in this conversation. The only thing you will be given is this message that you are now constructing with this outline as a template. Okay.

And then what I'll do is I'll create probably, like, I don't know how many I'm gonna do. Ideally, it'd be small. Ideally, it'd be like 5. 5 separate chat threads that are all tied to the same project. Right?

These are all within Arno7, Laney's project. But each individual new chat thread I create will have this user message as it's basically, it's like mission brief or whatever, for what we're doing. And then I will only upload between 5 and 10, or like 20, probably between, like, gosh, dude, between 5 and 20 blog post files to each chat thread, ideally more. And it doesn't have to all be once pertain. Right?

Like, I might say, "Hey, I'm gonna give you 5 now, and then we're gonna. You're gonna produce the content for the database based on these 5. And then I'm gonna upload 5 more." Right? You can do it like that.

But ideally, we wanna approach the context window limitations, which is 200k context tokens. So, yeah, ideally. Ideally, closer to 20 individual blog post files per chat thread. I. But again, maybe do 5 at a time just to see how.

See if we can iteratively reach that limit instead of reaching it all at once.

Because, again, okay, this makes sense to actually, you do want to add 5 at a time for a goal of 20 total, because I just read this on the Anthropic docs. What Claude does is before it responds to your very next message, it literally reads through the entire chat thread history, including any files you've uploaded. So if you upload all 20 blog post files at the very start, every single additional message you send in that chat thread, it will literally force itself to read through all 20 of those blog posts, regardless of whether they're relevant or not, regardless of whether you're trying to create content database information for all 20 at once. It will force read all 20 of those before answering your question. And that's what fills up the context window.

So instead, just add 5 at the beginning and say, "Hey, we're going to do this in batches of 5" and then have Claude, like, output all the content databases, like CSV file information for those 5 posts, say, "Great, now we're going to go and do the next 5," right? And that way I won't have to read all 20. I'll only read the 5 relevant ones that it's doing for that particular batch. That makes more sense. Okay.

The only alternative I was thinking of for this, and this might be something to save for another day, is basically doing the same exact thing within Cursor. So basically I go to Claude the web app and I get it to output the same like mission brief set of instructions as the first user message. I copy and paste that first message over to Cursor. I make sure that all, I make sure that all like 82 of the updated blog post with updated file names are within a folder called like Laney's blog post, same directory path as like, all my other content, like the bin content and all that, whatever it needs to be in. And then I again initiate a brand new Cursor AI chat where I give it that initial user message, which is like the mission brief.

And then I say, "Okay, cool, I want you to, again, you could, I guess you could use a Python script to write the content to a CSV file. That might be easier. I don't know, dude."

At the end of the day, like copying and pasting, because if you do 5 documents at a time with a web chat UI, you're gonna end up doing, what was that, like 16, copying and pasting 16 different times to one CSV file in the end, which really isn't bad. If you were to. Yeah, I could. That's not terrible. If you were to write, if you were to write that content, though, you would copy and paste zero times.

If you're writing that content to a CSV file and you just have a simple Python script that takes and like, writes that next content to one specific CSV file, you'd copy and paste zero times. But I'm just wondering if the hassle of setting up that stupid CSV file script in Python and like, all the implications of that, of accidentally rewriting or like, overwriting content would even be worth it.

I don't know, man. We'll see. Freaking see. So anyway, well, I'll choose which option I want to do at that point. I think the main thing though, before we do this.

So probably here's the deal. Before I even create like a mission brief user message to like, initialize new chat threads with, I want to make sure that I get down like a solid rhetorical structure for these Instagram post captions.

Right? And I think the only way to do that is probably by, I don't know, probably the best way to do that is take my current content database, like Claude chat thread, maybe I upload like 5 sample documents to it. I'd say 5 to 10. 5 to 10, just to give it a decent sample size.

And I would definitely include the 3 blog posts that I've written so far. But then I would also include at least, probably at least like 3 to 7 of Laney's other blog posts that I haven't ghost written, just to give it a good sample size with some edge cases. And I would just back and forth collaborate with Claude on. "Hey, how are we going to like, what are some rhetorical structures that we could use that would make sense for this type of content?" Right?

That's going to like, it's going to work across all of our most common blog post content that has adequate, like, solid, repurposable content. But also it's going to work for some of those educated where the blog post content might not be as beefy, or it's just written in a scatterbrained way, or it's not as obvious which parts to, like, pull and use in the rhetorical structure. I don't know. We just want to make sure we account for mainstream cases and then also edge cases. So yeah, let's do that.

So we'll do include my 3 current blog posts, which obviously are structured and formatted in a way that makes the rhetorical structure easy to write. And then, so I'll write a practice rhetorical structure for that. Like, that will work on all 3 of my blog posts, regardless of what proven approach they use, like benefits or bad habits or questions or tips, examples, mistakes, all this kind of stuff. So that's the thing, is the rhetorical structure should work regardless of which type of what type of proven approach is inherently given in the book, in the original blog post.

And ideally, I mean, we probably just want to start out with 1 rhetorical structure template. But if we can, if we get that down and it makes sense, we almost do like a revolver menu. Let the AI pick as more like a revolver, where you have 3 different menu options, like 3 different options of rhetorical structures you could potentially use, and these 3 are all on the menu. And Claude, like, based on the original blog post content, will select or choose which of the 3 rhetorical structure template options are as most conducive to that particular blog post content like which one aligns best and makes sense for that one. Again, I don't want to get too like heavy into this, but I think this could be a really cool, useful prompt template for the future.

Just that principle of a revolver. Or like a revolving selection of menu items where the AI chooses based on the context, and then also, at least initially, will output its thinking or reasoning for why it chose that particular option off the menu. That'd be really cool.

But again, the 3 menu options isn't a requirement. This is more just like a cool second layer step to apply if the 1 menu option is already working.

Also, also, another thing that's really important to consider is tokens. Okay? If I use Cursor, if I use Cursor that's going to burn through. I think I have $2.84 left in my, in my like token credits, whatever you want to call it. So whatever.

If I'm doing a content database, I think it'd be important is if I get a token count on all those 82 input files. Because if I'm going to upload 82 blog post markdown files as knowledge base files, I need to know how many tokens, collectively those 82 blog post files are going to cost as input tokens, right? Because that itself could burn up a ton, ton of whatchamacallit, a ton of my token credits. And then, you know, if I run out of token credits, game over, man. Also, that method doesn't work anymore.

So honestly, I think I already know though, because if you're going to, if Claude tokens, context Windows 200K tokens, and I know what percentage of the project knowledge base files like the capacity that those 82 files fill up, which I want to say I don't fill it up to 97%, but I think pre existing files in that knowledge base were like 37%. So if that fills up like 60% of 220, 20,000 tokens, approximately. So then you need to estimate, okay, how much does 120,000 input tokens cost?

Yeah, anyway, I think Cursor is probably not the way to go for this because there's too many unaccounted variables and like configuration settings that I don't know how to do yet where I will, I guarantee you if I go into it now, I will accidentally burn up my entire token credit allotment by accidentally having a read through all the files multiple times or doing something stupid like that. So I think probably the best way to set safe guardrails for this is just take, take the trade off of having to copy and paste, you know, potentially 16 different CSV files into 1 CSV file and use the Claude AI web app user interface. That way it'll like, you know, it'll either rate limit me or warning or heads up when I've burned through my token alert via the Claude pro plan, right? That way it's not costing me money. It's like I already paid my $20, it'll just re limit me.

And then in future iterations or versions, if I really want to, I can figure out how to replicate this whole process using Cursor AI, using Python scripts and all that to make it more efficient. But yeah, ultimately we got a first off, we got to check to make sure we've got got a solid rhetorical structure that we can use to transform each blog post into a solid Instagram post and then create a user message as sort of like a mission briefing sort of intro message for individual chat threads that are all related to the project but will batch process between 5 and 20 individual blog posts. Two CSV files that are then copy and paste into 1 CSV file ultimately import into Notion at the end.

<!-- transcript complete -->